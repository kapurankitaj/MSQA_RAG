import os
import json
import numpy as np
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging
import traceback

# Required for RAGAS evaluation
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall
)

from ragas import evaluate
from datasets import Dataset

# ============= CONFIGURATION SETTINGS =============
CONFIG = {
    # Paths
    "paths": {
        "data_dir": "data",
        "evaluation_dir": "data/evaluation",
        "ground_truth_path": "data/evaluation/ground_truth_data.json",
        "evaluation_results_path": "data/evaluation/evaluation_results.json",
        "report_path": "data/evaluation/evaluation_report.html"
    },
    
    # Evaluation settings
    "evaluation": {
        "metrics": [
            "faithfulness",
            "answer_relevancy", 
            "context_precision",
            "context_recall"
        ],
        "batch_size": 5,  # Number of examples to evaluate in a batch
        "min_samples": 10,  # Minimum number of samples for meaningful evaluation
        "max_samples": 100  # Maximum number of samples to evaluate
    },
    
    # Reporting settings
    "score_thresholds": {
        "faithfulness": {"poor": 0.3, "fair": 0.7, "good": 0.9},
        "answer_relevancy": {"poor": 0.3, "fair": 0.7, "good": 0.9},
        "context_precision": {"poor": 0.3, "fair": 0.7, "good": 0.9},
        "context_recall": {"poor": 0.3, "fair": 0.7, "good": 0.9}
    },
    
    # Logging settings
    "logging": {
        "level": logging.INFO,
        "format": '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        "log_file": "ragas_evaluation.log"
    }
}

# Set up logging
logging.basicConfig(
    level=CONFIG["logging"]["level"],
    format=CONFIG["logging"]["format"],
    handlers=[
        logging.FileHandler(CONFIG["logging"]["log_file"]),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("RAGASEvaluation")

class RAGASEvaluator:
    """
    Implements custom evaluation metrics for RAG systems
    """
    
    def __init__(self, config=CONFIG):
        """
        Initialize the evaluator
        
        Args:
            config: Configuration dictionary
        """
        self.config = config
        self.metrics = config["evaluation"]["metrics"]
        self.batch_size = config["evaluation"]["batch_size"]
        self.results = {}
        self.dataset = None
        
        # Initialize evaluation directory
        os.makedirs(config["paths"]["evaluation_dir"], exist_ok=True)
        
        logger.info(f"Initialized RAG Evaluator")
        logger.info(f"Metrics to evaluate: {', '.join(self.metrics)}")
    
    def create_evaluation_dataset(
        self,
        questions: List[str],
        generated_answers: List[str],
        contexts: List[List[str]],
        ground_truths: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Create an evaluation dataset
        
        Args:
            questions: List of questions
            generated_answers: Corresponding answers generated by the RAG system
            contexts: Contexts used to generate each answer
            ground_truths: Optional ground truth answers
            
        Returns:
            Dictionary with dataset information
        """
        # Validate input
        if not (len(questions) == len(generated_answers) == len(contexts)):
            raise ValueError("Number of questions, answers, and contexts must match")
        
        if ground_truths and len(questions) != len(ground_truths):
            raise ValueError("Number of questions and ground truths must match")
        
        # Store dataset
        self.dataset = {
            "questions": questions,
            "answers": generated_answers,
            "contexts": contexts,
            "ground_truths": ground_truths or []
        }
        
        logger.info(f"Created evaluation dataset with {len(questions)} examples")
        return self.dataset
    
    def save_dataset(self, path: Optional[str] = None):
        """
        Save the evaluation dataset to disk
        
        Args:
            path: Optional path to save the dataset (defaults to config path)
        """
        if not self.dataset:
            raise ValueError("No dataset to save. Create one first with create_evaluation_dataset.")
        
        save_path = path or self.config["paths"]["ground_truth_path"]
        
        try:
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(self.dataset, f, indent=2)
            
            logger.info(f"Saved evaluation dataset to {save_path}")
        except Exception as e:
            logger.error(f"Error saving dataset: {e}")
    
    def load_dataset(self, path: Optional[str] = None):
        """
        Load evaluation dataset from disk
        
        Args:
            path: Optional path to load the dataset from (defaults to config path)
        """
        load_path = path or self.config["paths"]["ground_truth_path"]
        
        try:
            with open(load_path, 'r', encoding='utf-8') as f:
                self.dataset = json.load(f)
            
            logger.info(f"Loaded evaluation dataset from {load_path}")
            logger.info(f"Dataset contains {len(self.dataset['questions'])} examples")
        except Exception as e:
            logger.error(f"Error loading dataset: {e}")
            self.dataset = None
    
    def _calculate_metric_score(
        self, 
        metric_name: str, 
        answers: List[str], 
        contexts: List[List[str]], 
        questions: List[str],
        ground_truths: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Calculate a specific metric score
        
        Args:
            metric_name: Name of the metric to calculate
            answers: Generated answers
            contexts: Contexts used
            questions: Original questions
            ground_truths: Optional ground truth answers
            
        Returns:
            Dictionary with metric scores
        """
        try:
            # Create a HuggingFace dataset for RAGAS
            data_dict = {
                "question": questions,
                "answer": answers,
                "contexts": [ctx for ctx in contexts],
            }
            
            if ground_truths and len(ground_truths) == len(questions):
                data_dict["ground_truth"] = ground_truths
            
            # Create dataset
            hf_dataset = Dataset.from_dict(data_dict)
            
            # Select appropriate metric
            metric_fn = None
            if metric_name == "faithfulness":
                metric_fn = faithfulness
            elif metric_name == "answer_relevancy":
                metric_fn = answer_relevancy
            elif metric_name == "context_precision":
                metric_fn = context_precision
            elif metric_name == "context_recall":
                metric_fn = context_recall
            else:
                raise ValueError(f"Unknown metric: {metric_name}")
            
            # Evaluate
            logger.info(f"Computing {metric_name} metric...")
            try:
                result = evaluate(hf_dataset, [metric_fn])
                
                # Extract scores
                score_values = result[metric_name].tolist() if hasattr(result[metric_name], 'tolist') else result[metric_name]
                
                return {
                    "mean": float(np.mean(score_values)),
                    "std": float(np.std(score_values)),
                    "min": float(np.min(score_values)),
                    "max": float(np.max(score_values)),
                    "values": [float(s) for s in score_values]
                }
            except Exception as eval_error:
                # Fallback to mock scoring if RAGAS evaluation fails
                logger.warning(f"RAGAS evaluation failed: {eval_error}. Using mock scoring instead.")
                return self._mock_metric_score(metric_name, answers, contexts, questions)
                
        except Exception as e:
            logger.error(f"Error calculating {metric_name}: {e}")
            traceback.print_exc()
            return {}
    
    def _mock_metric_score(
        self, 
        metric_name: str, 
        answers: List[str], 
        contexts: List[List[str]], 
        questions: List[str]
    ) -> Dict[str, Any]:
        """
        Generate mock scores for testing or when RAGAS fails
        
        Args:
            metric_name: Name of the metric to mock
            answers: Generated answers
            contexts: Contexts used
            questions: Original questions
            
        Returns:
            Dictionary with mock metric scores
        """
        # Simple mock scoring mechanism
        np.random.seed(42)  # For reproducibility
        
        # Generate random scores based on metric
        if metric_name == "faithfulness":
            # Higher score if answer seems derived from context
            scores = [
                1.0 if any(ctx.lower() in ans.lower() for ctx in contexts[i]) 
                else np.random.uniform(0.3, 0.7) 
                for i, ans in enumerate(answers)
            ]
        elif metric_name == "answer_relevancy":
            # Higher score if answer length is reasonable and relates to question
            scores = [
                1.0 if 10 < len(ans) < 200 and any(q.lower() in ans.lower() for q in questions) 
                else np.random.uniform(0.3, 0.7) 
                for ans in answers
            ]
        elif metric_name == "context_precision":
            # Score based on context relevance
            scores = [
                1.0 if len(ctx) > 0 and any(key.lower() in ' '.join(ctx).lower() for key in questions) 
                else np.random.uniform(0.3, 0.7) 
                for ctx in contexts
            ]
        elif metric_name == "context_recall":
            # Score based on context comprehensiveness
            scores = [
                1.0 if len(ctx) > 1 else np.random.uniform(0.3, 0.7) 
                for ctx in contexts
            ]
        else:
            # Default random score
            scores = list(np.random.uniform(0.3, 0.9, len(answers)))
        
        logger.info(f"Generated mock scores for {metric_name}")
        
        return {
            "mean": float(np.mean(scores)),
            "std": float(np.std(scores)),
            "min": float(np.min(scores)),
            "max": float(np.max(scores)),
            "values": [float(s) for s in scores],
            "is_mock": True
        }
    
    def run_evaluation(self, metrics: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Run evaluation on the dataset
        
        Args:
            metrics: Optional list of metrics to evaluate (overrides default)
        
        Returns:
            Dictionary with evaluation results
        """
        if not self.dataset:
            raise ValueError("No dataset created. Use create_evaluation_dataset first.")
        
        try:
            # Use provided metrics or default to class metrics
            eval_metrics = metrics or self.metrics
            
            # Calculate metrics
            results = {}
            for metric in eval_metrics:
                results[metric] = self._calculate_metric_score(
                    metric_name=metric,
                    answers=self.dataset["answers"],
                    contexts=self.dataset["contexts"],
                    questions=self.dataset["questions"],
                    ground_truths=self.dataset.get("ground_truths")
                )
            
            # Add metadata
            results["metadata"] = {
                "timestamp": datetime.now().isoformat(),
                "metrics": eval_metrics,
                "num_samples": len(self.dataset["answers"])
            }
            
            self.results = results
            
            # Save results
            self._save_results()
            
            return results
        
        except Exception as e:
            logger.error(f"Error in evaluation: {e}")
            traceback.print_exc()
            return {}
    
    def _save_results(self, path: Optional[str] = None):
        """
        Save evaluation results to disk
        
        Args:
            path: Optional path to save results (defaults to config path)
        """
        if not self.results:
            logger.warning("No results to save")
            return
        
        save_path = path or self.config["paths"]["evaluation_results_path"]
        
        try:
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2)
            
            logger.info(f"Saved evaluation results to {save_path}")
        except Exception as e:
            logger.error(f"Error saving results: {e}")
    
    def generate_report(self, path: Optional[str] = None) -> str:
        """
        Generate a human-readable report of evaluation results
        
        Args:
            path: Optional path to save the report (defaults to config path)
        
        Returns:
            Path to the saved report
        """
        if not self.results:
            raise ValueError("No results to report. Run evaluation first.")
        
        report_path = path or self.config["paths"]["report_path"]
        
        try:
            # Create simple HTML report
            html_content = ["<html>", "<head>", 
                           "<title>RAG Evaluation Report</title>",
                           "<style>",
                           "body { font-family: Arial, sans-serif; margin: 20px; }",
                           "h1, h2 { color: #2c3e50; }",
                           "table { border-collapse: collapse; width: 100%; }",
                           "th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }",
                           "th { background-color: #f2f2f2; }",
                           "tr:nth-child(even) { background-color: #f9f9f9; }",
                           ".good { color: green; }",
                           ".fair { color: orange; }",
                           ".poor { color: red; }",
                           "</style>",
                           "</head>", 
                           "<body>",
                           f"<h1>RAG Evaluation Report</h1>",
                           f"<p>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>"]
            
            # Add summary table
            html_content.extend([
                "<h2>Summary</h2>",
                "<table>",
                "<tr><th>Metric</th><th>Mean Score</th><th>Rating</th></tr>"
            ])
            
            thresholds = self.config["score_thresholds"]
            
            for metric, result in self.results.items():
                if metric != "metadata" and "mean" in result:
                    mean_score = result["mean"]
                    # Determine rating
                    if metric in thresholds:
                        if mean_score >= thresholds[metric]["good"]:
                            rating = "Good"
                            rating_class = "good"
                        elif mean_score >= thresholds[metric]["fair"]:
                            rating = "Fair"
                            rating_class = "fair"
                        else:
                            rating = "Poor"
                            rating_class = "poor"
                    else:
                        rating = "N/A"
                        rating_class = ""
                    
                    html_content.append(
                        f"<tr><td>{metric.replace('_', ' ').title()}</td>"
                        f"<td>{mean_score:.3f}</td>"
                        f"<td class='{rating_class}'>{rating}</td></tr>"
                    )
            
            html_content.append("</table>")
            
            # Add details for each metric
            html_content.append("<h2>Detailed Results</h2>")
            for metric, result in self.results.items():
                if metric != "metadata" and "mean" in result:
                    html_content.extend([
                        f"<h3>{metric.replace('_', ' ').title()}</h3>",
                        "<table>",
                        "<tr><th>Statistic</th><th>Value</th></tr>",
                        f"<tr><td>Mean</td><td>{result['mean']:.3f}</td></tr>",
                        f"<tr><td>Standard Deviation</td><td>{result['std']:.3f}</td></tr>",
                        f"<tr><td>Min</td><td>{result['min']:.3f}</td></tr>",
                        f"<tr><td>Max</td><td>{result['max']:.3f}</td></tr>",
                        "</table>"
                    ])
                    
                    # Note if mock data was used
                    if result.get("is_mock", False):
                        html_content.append("<p><em>Note: Mock scoring was used for this metric.</em></p>")
            
            # Add metadata
            metadata = self.results.get("metadata", {})
            html_content.extend([
                "<h2>Metadata</h2>",
                "<table>",
                "<tr><th>Field</th><th>Value</th></tr>",
                f"<tr><td>Number of Samples</td><td>{metadata.get('num_samples', 'N/A')}</td></tr>",
                f"<tr><td>Timestamp</td><td>{metadata.get('timestamp', 'N/A')}</td></tr>",
                f"<tr><td>Metrics</td><td>{', '.join(metadata.get('metrics', []))}</td></tr>",
                "</table>"
            ])
            
            html_content.extend(["</body>", "</html>"])
            
            # Write HTML to file
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("\n".join(html_content))
            
            logger.info(f"Generated evaluation report at {report_path}")
            return report_path
        
        except Exception as e:
            logger.error(f"Error generating report: {e}")
            traceback.print_exc()
            return ""

def create_sample_ground_truth():
    """
    Create a sample ground truth dataset for testing
    
    Returns:
        Tuple of questions, answers, and contexts
    """
    questions = [
        "What is Retrieval Augmented Generation?",
        "How does vector search work?",
        "What is the difference between BM25 and semantic search?",
        "Why is citation tracking important in RAG systems?",
        "What metrics can be used to evaluate RAG systems?"
    ]
    
    ground_truth_answers = [
        "Retrieval Augmented Generation (RAG) is an AI framework that enhances large language models by incorporating external knowledge retrieval.",
        "Vector search works by converting text into high-dimensional vectors using embedding models, then finding similar vectors using distance metrics.",
        "BM25 is a keyword-based retrieval algorithm that ranks documents based on term frequency, while semantic search uses vector embeddings to capture meaning and context.",
        "Citation tracking is important in RAG systems because it ensures transparency, verifiability, and attributability of generated information.",
        "RAG systems can be evaluated using metrics like faithfulness, answer relevancy, context precision, and context recall."
    ]
    
    contexts = [
        [
            "Retrieval Augmented Generation (RAG) is a technique in artificial intelligence that combines retrieval-based methods with generative models.",
            "RAG enhances large language models by incorporating external knowledge sources."
        ],
        [
            "Vector search converts text into high-dimensional vectors using embedding models.",
            "These vectors capture semantic meaning, enabling similar concepts to be positioned close together in the vector space."
        ],
        [
            "BM25 is a traditional keyword-based ranking function that uses term frequency and inverse document frequency.",
            "Semantic search uses vector embeddings to capture the meaning and context of text, rather than relying solely on keywords."
        ],
        [
            "Citation tracking in RAG systems links generated information to its source documents.",
            "This provides transparency and allows users to verify the accuracy of information."
        ],
        [
            "RAG systems can be evaluated using metrics from RAGAS framework, including faithfulness, answer relevancy, and context precision.",
            "These metrics help assess the quality and reliability of generated responses."
        ]
    ]
    
    return questions, ground_truth_answers, contexts

def create_sample_responses():
    """
    Create sample responses for testing
    
    Returns:
        List of generated answers
    """
    return [
        "Retrieval Augmented Generation (RAG) is an AI technique that enhances language models by incorporating external knowledge retrieval.",
        "Vector search works by converting text into vectors using embedding models, finding similar vectors in a database.",
        "BM25 is a keyword-based algorithm that ranks documents based on term frequency, while semantic search uses vector embeddings to understand meaning.",
        "Citation tracking is important in RAG systems because it allows users to verify the sources of information and increases transparency.",
        "RAG systems can be evaluated using several metrics including faithfulness, answer relevancy, context precision and context recall."
    ]

def test_ragas_evaluator():
    """
    Test the RAG Evaluator with sample data
    """
    print("\n" + "=" * 50)
    print("RAG METRICS IMPLEMENTATION DEMONSTRATION")
    print("=" * 50)
    
    # Initialize evaluator
    evaluator = RAGASEvaluator()
    
    # Create sample data
    print("\nGenerating sample evaluation data...")
    questions, ground_truths, contexts = create_sample_ground_truth()
    generated_answers = create_sample_responses()
    
    # Create evaluation dataset
    print("\nCreating evaluation dataset...")
    dataset = evaluator.create_evaluation_dataset(
        questions=questions,
        generated_answers=generated_answers,
        contexts=contexts,
        ground_truths=ground_truths
    )
    
    # Run full evaluation
    print("\nRunning evaluation...")
    results = evaluator.run_evaluation()
    
    # Print results
    print("\nEvaluation Results:")
    for metric, scores in results.items():
        if isinstance(scores, dict) and 'mean' in scores:
            print(f"{metric.replace('_', ' ').title()}: {scores['mean']:.3f}")
    
    # Generate report
    print("\nGenerating evaluation report...")
    report_path = evaluator.generate_report()
    print(f"Report generated at: {report_path}")
    
    return evaluator, results

if __name__ == "__main__":
    test_ragas_evaluator()